# SCD Type 2 with Delta Lake MERGE
name: scd_type2_merge
description: Convert SSIS Slowly Changing Dimension Type 2 to Delta Lake MERGE
tags: [scd, type2, dimension, delta, merge]

input:
  type: ssis_component
  component_type: SlowlyChangingDimension
  content: |
    <component componentClassID="Microsoft.SlowlyChangingDimension" name="SCD - Customer Dimension">
      <properties>
        <property name="SqlCommand">SELECT * FROM dim.Customer WHERE IsCurrent = 1</property>
        <property name="CurrentRowWhere">IsCurrent = 1</property>
        <property name="EnableInferredMember">False</property>
      </properties>
      <inputs>
        <input name="Slowly Changing Dimension Input">
          <inputColumns>
            <!-- Business Key -->
            <inputColumn refId="CustomerID">
              <properties><property name="ColumnType">0</property></properties>
            </inputColumn>
            <!-- Historical Attributes (Type 2) -->
            <inputColumn refId="CustomerSegment">
              <properties><property name="ColumnType">2</property></properties>
            </inputColumn>
            <inputColumn refId="CreditLimit">
              <properties><property name="ColumnType">2</property></properties>
            </inputColumn>
            <!-- Changing Attributes (Type 1) -->
            <inputColumn refId="CustomerName">
              <properties><property name="ColumnType">1</property></properties>
            </inputColumn>
            <inputColumn refId="Email">
              <properties><property name="ColumnType">1</property></properties>
            </inputColumn>
          </inputColumns>
        </input>
      </inputs>
    </component>

output:
  type: pyspark
  content: |
    from delta.tables import DeltaTable
    from pyspark.sql import functions as F
    from pyspark.sql.functions import col, lit, current_timestamp, hash

    # Configuration
    dim_table_name = f"{catalog}.{schema}.dim_customer"
    business_key = "customer_id"
    type2_columns = ["customer_segment", "credit_limit"]
    type1_columns = ["customer_name", "email", "phone", "address"]

    # Get target Delta table
    dim_table = DeltaTable.forName(spark, dim_table_name)

    # Prepare source with SCD metadata
    df_source_prepared = df_source \
        .withColumn("_hash_type2", hash(*[col(c) for c in type2_columns]))

    # ===========================================================
    # STEP 1: Expire old records where Type 2 attributes changed
    # ===========================================================
    dim_table.alias("t").merge(
        df_source_prepared.alias("s"),
        f"t.{business_key} = s.{business_key} AND t.is_current = true"
    ).whenMatchedUpdate(
        condition=f"t._hash_type2 != s._hash_type2",
        set={
            "is_current": "false",
            "effective_end_date": "current_timestamp()",
            "modified_date": "current_timestamp()"
        }
    ).execute()

    # ===========================================================
    # STEP 2: Insert new records (new customers + new versions)
    # ===========================================================
    new_records = df_source_prepared \
        .withColumn("effective_start_date", current_timestamp()) \
        .withColumn("effective_end_date", lit("9999-12-31").cast("timestamp")) \
        .withColumn("is_current", lit(True)) \
        .withColumn("created_date", current_timestamp()) \
        .withColumn("modified_date", lit(None).cast("timestamp"))

    dim_table.alias("t").merge(
        new_records.alias("s"),
        f"t.{business_key} = s.{business_key} AND t.is_current = true"
    ).whenNotMatched().insertAll() \
    .execute()

    # ===========================================================
    # STEP 3: Update Type 1 attributes (in-place update)
    # ===========================================================
    type1_update_set = {c: f"s.{c}" for c in type1_columns}
    type1_update_set["modified_date"] = "current_timestamp()"

    type1_condition = " OR ".join([f"t.{c} != s.{c}" for c in type1_columns])

    dim_table.alias("t").merge(
        df_source_prepared.alias("s"),
        f"t.{business_key} = s.{business_key} AND t.is_current = true"
    ).whenMatchedUpdate(
        condition=type1_condition,
        set=type1_update_set
    ).execute()

    # Log SCD metrics
    print(f"SCD Type 2 processing complete for {dim_table_name}")

notes: |
  SCD Column Types in SSIS:
  - Type 0 = Business Key (match condition)
  - Type 1 = Changing Attribute (update in place)
  - Type 2 = Historical Attribute (create new version)
  - Type 3 = Not commonly used (previous value column)

  Delta Lake MERGE is ideal for SCD:
  - ACID transactions ensure consistency
  - Time travel for historical queries
  - Efficient upsert operations
